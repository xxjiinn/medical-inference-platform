"""
main.py
ì—­í• : Worker í”„ë¡œì„¸ìŠ¤ë“¤ì„ ê´€ë¦¬í•˜ëŠ” ë§¤ë‹ˆì €.
      WORKER_COUNTë§Œí¼ worker.pyë¥¼ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰í•˜ê³ ,
      í¬ë˜ì‹œ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ì¬ì‹œì‘ (Supervisor ì—­í• ).
      Springì˜ ThreadPoolTaskExecutor ê´€ë¦¬ìì™€ ìœ ì‚¬í•œ ê°œë….
"""

import os
import sys
import time
import signal
import logging
import multiprocessing
from datetime import timedelta

import redis

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Django ì„¤ì • ì´ˆê¸°í™” (settings.WORKER_COUNT ì½ê¸° ìœ„í•´ í•„ìš”)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")
import django
django.setup()

from django.conf import settings

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="[Manager] %(message)s")


def _recover_stuck_jobs() -> None:
    """
    ë‘ ê°€ì§€ stuck ìƒíƒœë¥¼ ë³µêµ¬:

    1. IN_PROGRESS stuck (10ë¶„ ì´ˆê³¼):
       ì›Œì»¤ í¬ë˜ì‹œ(SIGKILL ë“±)ë¡œ ì¶”ë¡  ì¤‘ ë©ˆì¶˜ job.
       ê¸°ì¤€: updated_atì´ 10ë¶„ ì´ìƒ ì§€ë‚œ IN_PROGRESS job.

    2. QUEUED stuck (5ë¶„ ì´ˆê³¼):
       API ì„œë²„ í¬ë˜ì‹œë¡œ DB ìƒì„± í›„ Redis enqueueê°€ ìœ ì‹¤ëœ job.
       (ì •ìƒ íë¦„ì—ì„œ QUEUED â†’ IN_PROGRESSëŠ” ìˆ˜ì´ˆ ì´ë‚´ â€” 5ë¶„ ê²½ê³¼ ì‹œ ìœ ì‹¤ íŒë‹¨)
       image Redis TTL = 10ë¶„ì´ë¯€ë¡œ 5ë¶„ ê¸°ì¤€ ë³µêµ¬ ì‹œ ì´ë¯¸ì§€ ì—¬ì „íˆ ìœ íš¨.

    ë‘ ì¼€ì´ìŠ¤ ëª¨ë‘ ë™ì¼í•œ retry ì¹´ìš´í„°(retry:{job_id})ë¥¼ ê³µìœ .
    MAX_RETRIES ì´ˆê³¼ ì‹œ FAILED + DLQ ì²˜ë¦¬.
    """
    from django.utils import timezone
    from apps.jobs.models import InferenceJob
    from workers.redis_queue import enqueue, REDIS_URL, DLQ_KEY

    now = timezone.now()
    # IN_PROGRESS: updated_at ê¸°ì¤€ (ë§ˆì§€ë§‰ ìƒíƒœ ë³€ê²½ ì‹œê°)
    in_progress_threshold = now - timedelta(minutes=10)
    # QUEUED stuck: created_at ê¸°ì¤€ (ìƒì„± ì´í›„ í•œ ë²ˆë„ ì²˜ë¦¬ ì‹œì‘ ì•ˆ ë¨)
    queued_threshold = now - timedelta(minutes=5)

    stuck_in_progress = list(InferenceJob.objects.filter(
        status=InferenceJob.Status.IN_PROGRESS,
        updated_at__lt=in_progress_threshold,
    ))
    stuck_queued = list(InferenceJob.objects.filter(
        status=InferenceJob.Status.QUEUED,
        created_at__lt=queued_threshold,
    ))

    if not stuck_in_progress and not stuck_queued:
        return

    r = redis.from_url(REDIS_URL, decode_responses=True)

    # â”€â”€ IN_PROGRESS stuck ë³µêµ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if stuck_in_progress:
        logger.warning(f"â—ï¸ IN_PROGRESS stuck job {len(stuck_in_progress)}ê°œ ê°ì§€")
        for job in stuck_in_progress:
            retry_key = f"retry:{job.id}"
            attempt = r.incr(retry_key)   # ë³µêµ¬ ì‹œë„ íšŸìˆ˜ ì¦ê°€
            r.expire(retry_key, 3600)     # 1ì‹œê°„ í›„ ìë™ ì‚­ì œ

            if attempt > settings.MAX_RETRIES:
                job.status = InferenceJob.Status.FAILED
                job.save(update_fields=["status", "updated_at"])
                r.delete(retry_key)
                r.lpush(DLQ_KEY, job.id)
                r.ltrim(DLQ_KEY, 0, 999)  # DLQ ìƒí•œ 1000ê°œ ìœ ì§€
                logger.warning(
                    f"  âŒ Job {job.id} ì¬ì‹œë„ {settings.MAX_RETRIES}íšŒ ì´ˆê³¼ â†’ FAILED (DLQ)"
                )
            else:
                job.status = InferenceJob.Status.QUEUED
                job.save(update_fields=["status", "updated_at"])
                enqueue(job.id)
                logger.info(f"  â†©ï¸  Job {job.id} ì¬íì‰ ({attempt}/{settings.MAX_RETRIES})")

    # â”€â”€ QUEUED stuck ë³µêµ¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ì›ì¸: POST /v1/jobsì—ì„œ DB create ì„±ê³µ í›„ enqueue ì „ ì„œë²„ í¬ë˜ì‹œ
    # í•´ê²°: Redis íì— job_id ì¬ë“±ë¡ (DB statusëŠ” ì´ë¯¸ QUEUEDì´ë¯€ë¡œ ë³€ê²½ ë¶ˆí•„ìš”)
    if stuck_queued:
        logger.warning(f"â—ï¸ QUEUED stuck job {len(stuck_queued)}ê°œ ê°ì§€ (enqueue ìœ ì‹¤ ì¶”ì •)")
        for job in stuck_queued:
            retry_key = f"retry:{job.id}"
            attempt = r.incr(retry_key)   # ë³µêµ¬ ì‹œë„ íšŸìˆ˜ ì¦ê°€
            r.expire(retry_key, 3600)     # 1ì‹œê°„ í›„ ìë™ ì‚­ì œ

            if attempt > settings.MAX_RETRIES:
                job.status = InferenceJob.Status.FAILED
                job.save(update_fields=["status", "updated_at"])
                r.delete(retry_key)
                r.lpush(DLQ_KEY, job.id)
                logger.warning(
                    f"  âŒ QUEUED stuck Job {job.id} â†’ FAILED (DLQ)"
                )
            else:
                # DB statusëŠ” QUEUED ìœ ì§€ â€” Redis íì—ë§Œ ì¬ë“±ë¡
                enqueue(job.id)
                logger.info(
                    f"  â†©ï¸  QUEUED stuck Job {job.id} ì¬íì‰ ({attempt}/{settings.MAX_RETRIES})"
                )


def start_worker_process() -> multiprocessing.Process:
    """
    worker.pyì˜ run_worker()ë¥¼ ìƒˆ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰.
    ê° í”„ë¡œì„¸ìŠ¤ëŠ” ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  Redis íë¥¼ í´ë§.
    (í”„ë¡œì„¸ìŠ¤ ê°„ ë©”ëª¨ë¦¬ ê³µìœ  ì—†ìŒ â€” PyTorch ë©€í‹°í”„ë¡œì„¸ì‹± ì¶©ëŒ ë°©ì§€)
    """
    from workers.worker import run_worker

    # daemon=False: ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ Workerê°€ í˜„ì¬ Jobì„ ì™„ë£Œí•˜ê³  ì¢…ë£Œ
    p = multiprocessing.Process(target=run_worker, daemon=False)
    p.start()
    logger.info(f"âœ… Worker í”„ë¡œì„¸ìŠ¤ ì‹œì‘ â€” PID={p.pid}")
    return p


def run_manager():
    """
    ë§¤ë‹ˆì € ë©”ì¸ ë£¨í”„.
    1. WORKER_COUNTê°œ Worker í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    2. ì£¼ê¸°ì ìœ¼ë¡œ Worker ìƒíƒœ í™•ì¸
    3. í¬ë˜ì‹œëœ Worker ìë™ ì¬ì‹œì‘
    4. SIGTERM ìˆ˜ì‹  ì‹œ ëª¨ë“  Workerì— ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡ (Graceful Shutdown)
    """
    shutdown = False

    def handle_sigterm(signum, frame):
        """Docker stop ë˜ëŠ” kill ì‹œ SIGTERM ìˆ˜ì‹  -> ëª¨ë“  Worker ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡."""
        nonlocal shutdown
        logger.info("âš ï¸ SIGTERM ìˆ˜ì‹  â€” ëª¨ë“  Worker ì¢…ë£Œ ì‹œì‘")
        shutdown = True

    signal.signal(signal.SIGTERM, handle_sigterm)
    signal.signal(signal.SIGINT, handle_sigterm)  # Ctrl+Cë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬

    worker_count = settings.WORKER_COUNT
    logger.info(f"ğŸ”¥ ë§¤ë‹ˆì € ì‹œì‘ â€” Worker {worker_count}ê°œ ì‹¤í–‰")

    # ì´ˆê¸° Worker í”„ë¡œì„¸ìŠ¤ í’€ ìƒì„±
    # Springì˜ ThreadPoolTaskExecutor.setCorePoolSize()ì™€ ë™ì¼
    processes: list[multiprocessing.Process] = [
        start_worker_process() for _ in range(worker_count)
    ]

    # stuck job ë³µêµ¬ íƒ€ì´ë¨¸ (10ë¶„ë§ˆë‹¤ ì‹¤í–‰)
    RECOVERY_INTERVAL = 600
    _last_recovery = time.monotonic()

    # ë§¤ë‹ˆì € ëª¨ë‹ˆí„°ë§ ë£¨í”„
    while not shutdown:
        time.sleep(3)  # 3ì´ˆë§ˆë‹¤ Worker ìƒíƒœ ì ê²€

        for i, p in enumerate(processes):
            if not p.is_alive():
                # Workerê°€ ì˜ˆê¸°ì¹˜ ì•Šê²Œ ì¢…ë£Œë¨ (í¬ë˜ì‹œ) -> ìƒˆ í”„ë¡œì„¸ìŠ¤ë¡œ êµì²´
                logger.warning(
                    f"â—ï¸ Worker {i} í¬ë˜ì‹œ ê°ì§€ (PID={p.pid}, exit={p.exitcode}) â€” ì¬ì‹œì‘"
                )
                p.close()  # ì£½ì€ í”„ë¡œì„¸ìŠ¤ ë¦¬ì†ŒìŠ¤ í•´ì œ
                processes[i] = start_worker_process()

        # 10ë¶„ë§ˆë‹¤ stuck job ë³µêµ¬ ì‹¤í–‰ (IN_PROGRESS 10ë¶„â†‘ + QUEUED 5ë¶„â†‘)
        if time.monotonic() - _last_recovery >= RECOVERY_INTERVAL:
            _recover_stuck_jobs()
            _last_recovery = time.monotonic()

    # Graceful Shutdown: ëª¨ë“  Workerì— SIGTERM ì „ì†¡
    logger.info("âš ï¸ ëª¨ë“  Workerì— SIGTERM ì „ì†¡ ì¤‘...")
    for p in processes:
        if p.is_alive():
            p.terminate()

    # ê° Workerê°€ í˜„ì¬ Jobì„ ì™„ë£Œí•˜ê³  ì¢…ë£Œë  ë•Œê¹Œì§€ ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
    for p in processes:
        p.join(timeout=30)
        if p.is_alive():
            # 30ì´ˆ ë‚´ ì¢…ë£Œ ì•ˆ ë˜ë©´ ê°•ì œ ì¢…ë£Œ
            logger.warning(f"âŒ Worker PID={p.pid} 30ì´ˆ ë‚´ ë¯¸ì¢…ë£Œ â€” ê°•ì œ ì¢…ë£Œ")
            p.kill()

    logger.info("âœ… ëª¨ë“  Worker ì¢…ë£Œ ì™„ë£Œ â€” ë§¤ë‹ˆì € ì¢…ë£Œ")


if __name__ == "__main__":
    # multiprocessing spawn ë°©ì‹ ëª…ì‹œ (Docker Linux í™˜ê²½ í˜¸í™˜ì„±)
    multiprocessing.set_start_method("spawn", force=True)
    run_manager()
