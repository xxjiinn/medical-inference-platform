"""
main.py
ì—­í• : Worker í”„ë¡œì„¸ìŠ¤ë“¤ì„ ê´€ë¦¬í•˜ëŠ” ë§¤ë‹ˆì €.
      WORKER_COUNTë§Œí¼ worker.pyë¥¼ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰í•˜ê³ ,
      í¬ë˜ì‹œ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ì¬ì‹œì‘ (Supervisor ì—­í• ).
      Springì˜ ThreadPoolTaskExecutor ê´€ë¦¬ìì™€ ìœ ì‚¬í•œ ê°œë….
"""

import os
import sys
import time
import signal
import logging
import multiprocessing
from datetime import timedelta

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Django ì„¤ì • ì´ˆê¸°í™” (settings.WORKER_COUNT ì½ê¸° ìœ„í•´ í•„ìš”)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")
import django
django.setup()

from django.conf import settings

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="[Manager] %(message)s")


def _recover_stuck_jobs() -> None:
    """
    ì›Œì»¤ í¬ë˜ì‹œ ë“±ìœ¼ë¡œ IN_PROGRESSì—ì„œ ë©ˆì¶˜ jobì„ QUEUEDë¡œ ë˜ëŒë ¤ ì¬íì‰.
    ê¸°ì¤€: updated_atì´ 10ë¶„ ì´ìƒ ì§€ë‚œ IN_PROGRESS job.
    (ì •ìƒ ì¶”ë¡ ì€ EC2 ê¸°ì¤€ ìµœëŒ€ 2~3ì´ˆ ì´ë‚´ ì™„ë£Œ â€” 10ë¶„ì€ ì¶©ë¶„í•œ ì—¬ìœ )

    í˜¸ì¶œ ì£¼ê¸°: 10ë¶„ë§ˆë‹¤ (RECOVERY_INTERVAL).
    """
    from django.utils import timezone
    from apps.jobs.models import InferenceJob
    from workers.redis_queue import enqueue

    threshold = timezone.now() - timedelta(minutes=10)
    stuck = InferenceJob.objects.filter(
        status=InferenceJob.Status.IN_PROGRESS,
        updated_at__lt=threshold,
    )
    count = stuck.count()
    if count == 0:
        return

    logger.warning(f"â—ï¸ IN_PROGRESS stuck job {count}ê°œ ê°ì§€ â€” QUEUEDë¡œ ë˜ëŒë ¤ ì¬íì‰")
    for job in stuck:
        job.status = InferenceJob.Status.QUEUED
        job.save(update_fields=["status", "updated_at"])
        enqueue(job.id)
        logger.info(f"  â†©ï¸  Job {job.id} ì¬íì‰ ì™„ë£Œ")


def start_worker_process() -> multiprocessing.Process:
    """
    worker.pyì˜ run_worker()ë¥¼ ìƒˆ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰.
    ê° í”„ë¡œì„¸ìŠ¤ëŠ” ë…ë¦½ì ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  Redis íë¥¼ í´ë§.
    (í”„ë¡œì„¸ìŠ¤ ê°„ ë©”ëª¨ë¦¬ ê³µìœ  ì—†ìŒ â€” PyTorch ë©€í‹°í”„ë¡œì„¸ì‹± ì¶©ëŒ ë°©ì§€)
    """
    from workers.worker import run_worker

    # daemon=False: ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ Workerê°€ í˜„ì¬ Jobì„ ì™„ë£Œí•˜ê³  ì¢…ë£Œ
    p = multiprocessing.Process(target=run_worker, daemon=False)
    p.start()
    logger.info(f"âœ… Worker í”„ë¡œì„¸ìŠ¤ ì‹œì‘ â€” PID={p.pid}")
    return p


def run_manager():
    """
    ë§¤ë‹ˆì € ë©”ì¸ ë£¨í”„.
    1. WORKER_COUNTê°œ Worker í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    2. ì£¼ê¸°ì ìœ¼ë¡œ Worker ìƒíƒœ í™•ì¸
    3. í¬ë˜ì‹œëœ Worker ìë™ ì¬ì‹œì‘
    4. SIGTERM ìˆ˜ì‹  ì‹œ ëª¨ë“  Workerì— ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡ (Graceful Shutdown)
    """
    shutdown = False

    def handle_sigterm(signum, frame):
        """Docker stop ë˜ëŠ” kill ì‹œ SIGTERM ìˆ˜ì‹  -> ëª¨ë“  Worker ì¢…ë£Œ ì‹ í˜¸ ì „ì†¡."""
        nonlocal shutdown
        logger.info("âš ï¸ SIGTERM ìˆ˜ì‹  â€” ëª¨ë“  Worker ì¢…ë£Œ ì‹œì‘")
        shutdown = True

    signal.signal(signal.SIGTERM, handle_sigterm)
    signal.signal(signal.SIGINT, handle_sigterm)  # Ctrl+Cë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬

    worker_count = settings.WORKER_COUNT
    logger.info(f"ğŸ”¥ ë§¤ë‹ˆì € ì‹œì‘ â€” Worker {worker_count}ê°œ ì‹¤í–‰")

    # ì´ˆê¸° Worker í”„ë¡œì„¸ìŠ¤ í’€ ìƒì„±
    # Springì˜ ThreadPoolTaskExecutor.setCorePoolSize()ì™€ ë™ì¼
    processes: list[multiprocessing.Process] = [
        start_worker_process() for _ in range(worker_count)
    ]

    # stuck job ë³µêµ¬ íƒ€ì´ë¨¸ (10ë¶„ë§ˆë‹¤ ì‹¤í–‰)
    RECOVERY_INTERVAL = 600
    _last_recovery = time.monotonic()

    # ë§¤ë‹ˆì € ëª¨ë‹ˆí„°ë§ ë£¨í”„
    while not shutdown:
        time.sleep(3)  # 3ì´ˆë§ˆë‹¤ Worker ìƒíƒœ ì ê²€

        for i, p in enumerate(processes):
            if not p.is_alive():
                # Workerê°€ ì˜ˆê¸°ì¹˜ ì•Šê²Œ ì¢…ë£Œë¨ (í¬ë˜ì‹œ) -> ìƒˆ í”„ë¡œì„¸ìŠ¤ë¡œ êµì²´
                logger.warning(
                    f"â—ï¸ Worker {i} í¬ë˜ì‹œ ê°ì§€ (PID={p.pid}, exit={p.exitcode}) â€” ì¬ì‹œì‘"
                )
                p.close()  # ì£½ì€ í”„ë¡œì„¸ìŠ¤ ë¦¬ì†ŒìŠ¤ í•´ì œ
                processes[i] = start_worker_process()

        # 10ë¶„ë§ˆë‹¤ IN_PROGRESS stuck job ë³µêµ¬ ì‹¤í–‰
        if time.monotonic() - _last_recovery >= RECOVERY_INTERVAL:
            _recover_stuck_jobs()
            _last_recovery = time.monotonic()

    # Graceful Shutdown: ëª¨ë“  Workerì— SIGTERM ì „ì†¡
    logger.info("âš ï¸ ëª¨ë“  Workerì— SIGTERM ì „ì†¡ ì¤‘...")
    for p in processes:
        if p.is_alive():
            p.terminate()

    # ê° Workerê°€ í˜„ì¬ Jobì„ ì™„ë£Œí•˜ê³  ì¢…ë£Œë  ë•Œê¹Œì§€ ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
    for p in processes:
        p.join(timeout=30)
        if p.is_alive():
            # 30ì´ˆ ë‚´ ì¢…ë£Œ ì•ˆ ë˜ë©´ ê°•ì œ ì¢…ë£Œ
            logger.warning(f"âŒ Worker PID={p.pid} 30ì´ˆ ë‚´ ë¯¸ì¢…ë£Œ â€” ê°•ì œ ì¢…ë£Œ")
            p.kill()

    logger.info("âœ… ëª¨ë“  Worker ì¢…ë£Œ ì™„ë£Œ â€” ë§¤ë‹ˆì € ì¢…ë£Œ")


if __name__ == "__main__":
    # multiprocessing spawn ë°©ì‹ ëª…ì‹œ (Docker Linux í™˜ê²½ í˜¸í™˜ì„±)
    multiprocessing.set_start_method("spawn", force=True)
    run_manager()
