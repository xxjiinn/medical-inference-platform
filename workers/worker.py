"""
worker.py
ì—­í• : Redis íì—ì„œ jobì„ êº¼ë‚´ ë°°ì¹˜ ì¶”ë¡  í›„ ê²°ê³¼ë¥¼ DBì— ì €ì¥.

í•µì‹¬ ì„¤ê³„:
  - 30ms ë°°ì¹˜ ìœˆë„ìš°(micro-batching): ì²« jobì„ BRPOPìœ¼ë¡œ ê¸°ë‹¤ë¦° ë’¤
    30ms ë™ì•ˆ ì¶”ê°€ jobì„ ëª¨ì•„ í•œ ë²ˆì˜ forward passë¡œ ì²˜ë¦¬í•œë‹¤.
    CPUì—ì„œëŠ” forward passê°€ ì„ í˜• ì¦ê°€í•´ throughput ì´ë“ì€ ì—†ë‹¤.
    ì‹¤ì§ˆ íš¨ê³¼ëŠ” Nê°œ job ìƒíƒœ ì „í™˜ì„ ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ì²˜ë¦¬í•˜ëŠ” DB ì˜¤ë²„í—¤ë“œ ì ˆê°.
    GPUë¡œ ì „í™˜ ì‹œ ë°°ì¹˜ ë³‘ë ¬í™”ë¡œ throughputê³¼ latency ëª¨ë‘ ê°œì„ ëœë‹¤.
  - INFERENCE_ENGINE í™˜ê²½ë³€ìˆ˜ë¡œ PyTorch/ONNX ì—”ì§„ ì „í™˜ ì§€ì›
    (ONNXëŠ” í˜„ì¬ ëª¨ë¸ êµ¬ì¡°ìƒ ë³€í™˜ ë¶ˆê°€ â€” ADR-003 ì°¸ê³ ).
  - ì‹¤íŒ¨ jobì€ Redis ì¬ì‹œë„ ì¹´ìš´í„°ë¡œ ì¶”ì í•´ MAX_RETRIES ì´ˆê³¼ ì‹œ FAILED + DLQ ì²˜ë¦¬.
"""

import os
import sys
import time
import json
import signal
import logging
import redis
from django.db import transaction

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ë…ë¦½ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ í•„ìš”)
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Django ORM ì‚¬ìš©ì„ ìœ„í•œ ì´ˆê¸°í™” â€” ë°˜ë“œì‹œ ëª¨ë¸ import ì „ì— ì‹¤í–‰í•´ì•¼ í•¨
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")
import django
django.setup()

from django.conf import settings
from apps.jobs.models import InferenceJob, InferenceResult
from workers.redis_queue import collect_batch, enqueue, REDIS_URL, DLQ_KEY

# INFERENCE_ENGINE ì„¤ì •ì— ë”°ë¼ ë¡œë” ì„ íƒ
# "onnx"ë©´ OnnxLoader, ê·¸ ì™¸(ê¸°ë³¸ê°’ "pytorch")ë©´ ModelLoader ì‚¬ìš©
if settings.INFERENCE_ENGINE == "onnx":
    from workers.onnx_loader import get_onnx_loader as get_loader
else:
    from workers.model_loader import get_loader

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="[Worker %(process)d] %(message)s")


def log(event: str, **kwargs):
    """êµ¬ì¡°í™” ë¡œê·¸ ì¶œë ¥. job_id ë“± ì»¨í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ê¸°ë¡í•´ ì¶”ì  ê°€ëŠ¥í•˜ê²Œ í•¨."""
    logger.info(json.dumps({"event": event, **kwargs}, ensure_ascii=False))


# â”€â”€ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_image_bytes(sha256: str) -> bytes | None:
    """Worker í”„ë¡œì„¸ìŠ¤ê°€ image:{sha256} í˜•íƒœì˜ í‚¤ë¥¼ ìƒì„±í•˜ê³  Redisì—ì„œ ì´ë¯¸ì§€ bytes ì¡°íšŒ. ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì—†ìœ¼ë©´ None ë°˜í™˜."""
    r = redis.from_url(REDIS_URL, decode_responses=False)
    return r.get(f"image:{sha256}")


# â”€â”€ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _timeout_handler(signum, frame):
    """SIGALRM ìˆ˜ì‹  ì‹œ â€” ë°°ì¹˜ ì¶”ë¡  ì „ì²´ê°€ ì œí•œ ì‹œê°„ ì´ˆê³¼."""
    raise TimeoutError("Batch inference timed out")

signal.signal(signal.SIGALRM, _timeout_handler)


# â”€â”€ ë°°ì¹˜ ì²˜ë¦¬ í•µì‹¬ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_batch(job_ids: list[int]) -> None:
    """
    ì—¬ëŸ¬ jobì„ í•œ ë²ˆì˜ ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ ì²˜ë¦¬.

    ë‹¨ê³„:
      1. DBì—ì„œ Job ì¡°íšŒ + ìƒíƒœ IN_PROGRESSë¡œ ì¼ê´„ ì—…ë°ì´íŠ¸ 
      (job_idë¥¼ DBì—ì„œ ì¡°íšŒ -> input_sha256 ê°€ì ¸ì˜´. -> redisì—ì„œ ê·¸ input_sha256ìœ¼ë¡œ ì´ë¯¸ì§€ bytes ì¶”ì¶œ)
      2. Redisì—ì„œ ì´ë¯¸ì§€ bytes ê°€ì ¸ì™€ ì „ì²˜ë¦¬ (bytes -> tensor)
      3. ìœ íš¨í•œ í…ì„œë§Œ ë°°ì¹˜ë¡œ ë¬¶ì–´ ë‹¨ì¼ forward pass ì‹¤í–‰
      (ì¦‰, ì—¬ëŸ¬ ê°¸ì˜ í…ì„œë¥¼ í•˜ë‚˜ì˜ (N, 1, 224, 224) í˜•íƒœë¡œ ë§Œë“¤ì–´ì„œ í•œë²ˆì— ëª¨ë¸ì—ê²Œ ë³´ë‚´ëŠ” ê²ƒ)
      4. ê²°ê³¼((N, 18)í˜•íƒœì˜ í–‰ë ¬)ë¥¼ ê° jobì˜ InferenceResultë¡œ ì €ì¥ + COMPLETED ì²˜ë¦¬
      5. ì´ë¯¸ì§€ ì—†ê±°ë‚˜ ì „ì²˜ë¦¬ ì‹¤íŒ¨í•œ job -> ì¬ì‹œë„ ë˜ëŠ” FAILED ì²˜ë¦¬
    """
    loader = get_loader()

    # 1. DBì—ì„œ Job ê°ì²´ ì¡°íšŒ + IN_PROGRESSë¡œ ì›ìì  ì „í™˜
    #    select_for_update(skip_locked=True): ì´ë¯¸ ë‹¤ë¥¸ ì›Œì»¤ê°€ ì²˜ë¦¬ ì¤‘ì¸ rowëŠ” ê±´ë„ˆëœ€.
    #    WORKER_COUNT > 1 í™˜ê²½ì—ì„œ ê°™ì€ jobì´ ë‘ ì›Œì»¤ì—ê²Œ ë™ì‹œ ì²˜ë¦¬ë˜ëŠ” ê²ƒì„ DB ë½ìœ¼ë¡œ ë°©ì§€.
    #    transaction.atomic() ë¸”ë¡ ì•ˆì—ì„œ ë½ì„ ì¡ê³  ì¦‰ì‹œ IN_PROGRESSë¡œ ê°±ì‹  í›„ í•´ì œ.
    with transaction.atomic():
        locked = list(
            InferenceJob.objects.select_for_update(skip_locked=True).filter(
                pk__in=job_ids,
                status=InferenceJob.Status.QUEUED,  # ì´ë¯¸ IN_PROGRESSë¡œ ì„ ì ëœ job ì œì™¸
            )
        )
        if not locked:
            # ëª¨ë“  jobì´ ë‹¤ë¥¸ ì›Œì»¤ì—ê²Œ ì„ ì ë¨ (WORKER_COUNT > 1 ë™ì‹œ ì²˜ë¦¬ ìƒí™©)
            logger.warning(f"âš ï¸ job_ids={job_ids} ì „ë¶€ ë‹¤ë¥¸ ì›Œì»¤ì—ê²Œ ì„ ì ë¨ â€” ìŠ¤í‚µ")
            return

        jobs = {job.id: job for job in locked}

        # DBì— ì—†ëŠ” job_id ê²½ê³ 
        for jid in job_ids:
            if jid not in jobs:
                logger.warning(f"âš ï¸ Job {jid} DBì— ì—†ê±°ë‚˜ ì´ë¯¸ ì„ ì ë¨, ìŠ¤í‚µ")

        # ë½ ë³´ìœ  ì¤‘ IN_PROGRESSë¡œ ì¼ê´„ ê°±ì‹  â€” transaction ì»¤ë°‹ ì‹œ ë½ í•´ì œ
        InferenceJob.objects.filter(pk__in=list(jobs.keys())).update(
            status=InferenceJob.Status.IN_PROGRESS
        )
    batch_start = time.time()  # ë°°ì¹˜ ì „ì²´ ì²˜ë¦¬ ì‹œì‘ ì‹œê° ê¸°ë¡
    log("batch_start", job_ids=list(jobs.keys()), batch_size=len(jobs))

    # 3. ê° Jobì˜ ì´ë¯¸ì§€ bytesë¥¼ Redisì—ì„œ ê°€ì ¸ì™€ ì „ì²˜ë¦¬
    valid_jobs = []    # ì •ìƒì ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ (job, tensor) ìŒ
    failed_jobs = []   # ì´ë¯¸ì§€ ì—†ê±°ë‚˜ ì „ì²˜ë¦¬ ì‹¤íŒ¨í•œ job

    for job in jobs.values():
        image_bytes = fetch_image_bytes(job.input_sha256)
        if image_bytes is None:
            log("image_not_found", job_id=job.id, reason="redis_expired_or_missing")
            failed_jobs.append(job)
            continue
        try:
            tensor = loader.preprocess(image_bytes)  # bytes -> (1,1,224,224) tensor
            valid_jobs.append((job, tensor))
        except Exception as e:
            log("preprocess_failed", job_id=job.id, error=str(e))
            failed_jobs.append(job)

    # 4. ìœ íš¨í•œ jobë“¤ì„ ë°°ì¹˜ ì¶”ë¡ 
    if valid_jobs:
        tensors = [t for _, t in valid_jobs]  # tensor ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ

        # ë°°ì¹˜ ì¶”ë¡  íƒ€ì„ì•„ì›ƒ = ë‹¨ì¼ íƒ€ì„ì•„ì›ƒ Ã— ë°°ì¹˜ í¬ê¸°
        # (ë°°ì¹˜ê°€ í´ìˆ˜ë¡ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ë¹„ë¡€í•˜ì—¬ ì—¬ìœ ë¥¼ ì¤Œ)
        signal.alarm(settings.INFERENCE_TIMEOUT * len(tensors))
        try:
            # í•µì‹¬: Nê°œ í…ì„œë¥¼ (N,1,224,224)ë¡œ ë¬¶ì–´ í•œ ë²ˆì˜ forward pass ì‹¤í–‰
            batch_scores = loader.predict_batch(tensors)
        except TimeoutError:
            # ë°°ì¹˜ ì „ì²´ íƒ€ì„ì•„ì›ƒ -> ì „ë¶€ ê°œë³„ ì¬ì‹œë„ ëŒ€ìƒìœ¼ë¡œ ì´ë™
            log("inference_timeout", job_ids=[job.id for job, _ in valid_jobs])
            failed_jobs.extend(job for job, _ in valid_jobs)
            valid_jobs = []
            batch_scores = []
        except Exception as e:
            log("inference_error", job_ids=[job.id for job, _ in valid_jobs], error=str(e))
            failed_jobs.extend(job for job, _ in valid_jobs)
            valid_jobs = []
            batch_scores = []
        finally:
            signal.alarm(0)  # íƒ€ì´ë¨¸ í•´ì œ

        # 5. ë°°ì¹˜ ì¶”ë¡  ì„±ê³µí•œ jobë“¤ ê²°ê³¼ ì €ì¥
        for (job, _), scores in zip(valid_jobs, batch_scores):
            top_label = max(scores, key=lambda k: scores[k])
            InferenceResult.objects.create(job=job, output=scores, top_label=top_label)
            job.status = InferenceJob.Status.COMPLETED
            job.save(update_fields=["status", "updated_at"])
            latency_ms = round((time.time() - batch_start) * 1000, 1)
            log("inference_completed", job_id=job.id, top_label=top_label, latency_ms=latency_ms)

    # 6. ì‹¤íŒ¨ jobë“¤: ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬ í›„ ì²˜ë¦¬
    #    ì¬ì‹œë„ íšŸìˆ˜ëŠ” Redisì— ì¹´ìš´í„°ë¡œ ê´€ë¦¬ (DB ì¶”ê°€ ì»¬ëŸ¼ ì—†ì´)
    if failed_jobs:
        _handle_failed_jobs(failed_jobs)


def _handle_failed_jobs(jobs: list) -> None:
    """
    ì‹¤íŒ¨í•œ jobë“¤ì˜ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ Redis ì¹´ìš´í„°ë¡œ ì¶”ì .
    MAX_RETRIES ë¯¸ë§Œì´ë©´ íì— ì¬ë“±ë¡, ì´ˆê³¼ ì‹œ FAILED ì²˜ë¦¬.

    Redis ì¬ì‹œë„ ì¹´ìš´í„° í‚¤: retry:{job_id}  (TTL 1ì‹œê°„)
    """
    r = redis.from_url(REDIS_URL, decode_responses=True)

    for job in jobs:
        retry_key = f"retry:{job.id}"
        # INCR: ì¹´ìš´í„° ì—†ìœ¼ë©´ 0ì—ì„œ ì‹œì‘í•´ 1 ì¦ê°€, ìˆìœ¼ë©´ +1
        attempt = r.incr(retry_key)
        r.expire(retry_key, 3600)  # 1ì‹œê°„ í›„ ìë™ ì‚­ì œ

        if attempt <= settings.MAX_RETRIES:
            # ì¬ì‹œë„ ê°€ëŠ¥ -> í ë§¨ ë’¤ì— ë‹¤ì‹œ ë“±ë¡
            log("job_retry", job_id=job.id, attempt=f"{attempt}/{settings.MAX_RETRIES}")
            enqueue(job.id)
        else:
            # ì¬ì‹œë„ íšŸìˆ˜ ì†Œì§„ -> FAILED í™•ì •
            job.status = InferenceJob.Status.FAILED
            job.save(update_fields=["status", "updated_at"])
            r.delete(retry_key)  # ì¹´ìš´í„° ì •ë¦¬
            # Dead Letter Queueì— job_id ë³´ê´€ (ìš´ì˜ìê°€ ë‚˜ì¤‘ì— í™•ì¸/ì¬ì²˜ë¦¬ ê°€ëŠ¥)
            r.lpush(DLQ_KEY, job.id)
            # DLQ í¬ê¸° ìƒí•œ 1000ê°œ ìœ ì§€ â€” ë¬´ì œí•œ ëˆ„ì ìœ¼ë¡œ ì¸í•œ ë©”ëª¨ë¦¬ ì¦ê°€ ë°©ì§€
            r.ltrim(DLQ_KEY, 0, 999)
            log("job_failed", job_id=job.id, max_retries=settings.MAX_RETRIES, dlq=DLQ_KEY)


# â”€â”€ ì›Œì»¤ ë©”ì¸ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_worker():
    """
    ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ì¸ ë£¨í”„.
    ëª¨ë¸ ë¡œë“œ -> Redis í ë°°ì¹˜ í´ë§ -> ë°°ì¹˜ ì¶”ë¡  ë°˜ë³µ.
    SIGTERM ìˆ˜ì‹  ì‹œ í˜„ì¬ ë°°ì¹˜ ì™„ë£Œ í›„ ì¢…ë£Œ (Graceful Shutdown).
    """
    shutdown = False

    def handle_sigterm(signum, frame):
        nonlocal shutdown
        logger.info("âš ï¸ SIGTERM ìˆ˜ì‹  â€” í˜„ì¬ ë°°ì¹˜ ì™„ë£Œ í›„ ì¢…ë£Œ")
        shutdown = True

    signal.signal(signal.SIGTERM, handle_sigterm)

    # ëª¨ë¸ ë¡œë“œ (HuggingFace ìºì‹œ ë˜ëŠ” ë‹¤ìš´ë¡œë“œ í›„ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¼)
    loader = get_loader()
    loader.load()
    logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ â€” Worker ì¤€ë¹„ ã„±ã„±")

    while not shutdown:
        # 30ms ìœˆë„ìš°ë¡œ ë°°ì¹˜ ìˆ˜ì§‘ (ìµœëŒ€ 8ê°œ)
        # íê°€ ë¹„ë©´ BRPOPì´ 5ì´ˆ ëŒ€ê¸° í›„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        job_ids = collect_batch(
            max_wait_ms=settings.BATCH_WINDOW_MS,
            max_size=8,
        )

        if not job_ids:
            # íê°€ ë¹„ì–´ìˆìŒ â€” shutdown ì—¬ë¶€ ì²´í¬ í›„ ë‹¤ì‹œ ëŒ€ê¸°
            continue

        logger.info(f"ğŸ”¥ Batch ìˆ˜ì§‘: {job_ids}")
        process_batch(job_ids)

    logger.info("âœ… Worker ì •ìƒ ì¢…ë£Œ")


if __name__ == "__main__":
    run_worker()
