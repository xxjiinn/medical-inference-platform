"""
worker.py
ì—­í• : Redis íì—ì„œ job ë°°ì¹˜ë¥¼ ìˆ˜ì§‘í•´ í•œ ë²ˆì˜ forward passë¡œ ì¶”ë¡ í•˜ê³  ê²°ê³¼ë¥¼ DBì— ì €ì¥.
      Phase 6 Micro-batching: 30ms ìœˆë„ìš° ë‚´ jobë“¤ì„ ë¬¶ì–´ ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ ì²˜ë¦¬ íš¨ìœ¨ í–¥ìƒ.
      Phase 7: INFERENCE_ENGINE í™˜ê²½ë³€ìˆ˜ë¡œ PyTorch/ONNX ì—”ì§„ ì „í™˜ ì§€ì›.
      ì‹¤íŒ¨ jobì€ ê°œë³„ ì¬ì‹œë„(MAX_RETRIES) í›„ FAILED ì²˜ë¦¬.
"""

import os
import sys
import time
import json
import signal
import logging
import redis as redis_lib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ë…ë¦½ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ í•„ìš”)
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Django ORM ì‚¬ìš©ì„ ìœ„í•œ ì´ˆê¸°í™” â€” ë°˜ë“œì‹œ ëª¨ë¸ import ì „ì— ì‹¤í–‰í•´ì•¼ í•¨
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "config.settings")
import django
django.setup()

from django.conf import settings
from apps.jobs.models import InferenceJob, InferenceResult
from workers.redis_queue import collect_batch, REDIS_URL, DLQ_KEY

# INFERENCE_ENGINE ì„¤ì •ì— ë”°ë¼ ë¡œë” ì„ íƒ
# "onnx"ë©´ OnnxLoader, ê·¸ ì™¸(ê¸°ë³¸ê°’ "pytorch")ë©´ ModelLoader ì‚¬ìš©
if settings.INFERENCE_ENGINE == "onnx":
    from workers.onnx_loader import get_onnx_loader as get_loader
else:
    from workers.model_loader import get_loader

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="[Worker %(process)d] %(message)s")


def log(event: str, **kwargs):
    """êµ¬ì¡°í™” ë¡œê·¸ ì¶œë ¥. job_id ë“± ì»¨í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ê¸°ë¡í•´ ì¶”ì  ê°€ëŠ¥í•˜ê²Œ í•¨."""
    logger.info(json.dumps({"event": event, **kwargs}, ensure_ascii=False))


# â”€â”€ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_image_bytes(sha256: str) -> bytes | None:
    """Redisì—ì„œ ì´ë¯¸ì§€ bytes ì¡°íšŒ. ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì—†ìœ¼ë©´ None ë°˜í™˜."""
    r = redis_lib.from_url(REDIS_URL, decode_responses=False)
    return r.get(f"image:{sha256}")


# â”€â”€ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _timeout_handler(signum, frame):
    """SIGALRM ìˆ˜ì‹  ì‹œ â€” ë°°ì¹˜ ì¶”ë¡  ì „ì²´ê°€ ì œí•œ ì‹œê°„ ì´ˆê³¼."""
    raise TimeoutError("Batch inference timed out")

signal.signal(signal.SIGALRM, _timeout_handler)


# â”€â”€ ë°°ì¹˜ ì²˜ë¦¬ í•µì‹¬ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def process_batch(job_ids: list[int]) -> None:
    """
    ì—¬ëŸ¬ jobì„ í•œ ë²ˆì˜ ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ ì²˜ë¦¬.

    ë‹¨ê³„:
      1. DBì—ì„œ Job ì¡°íšŒ + ìƒíƒœ IN_PROGRESSë¡œ ì¼ê´„ ì—…ë°ì´íŠ¸
      2. Redisì—ì„œ ì´ë¯¸ì§€ bytes ê°€ì ¸ì™€ ì „ì²˜ë¦¬ (bytes -> tensor)
      3. ìœ íš¨í•œ í…ì„œë§Œ ë°°ì¹˜ë¡œ ë¬¶ì–´ ë‹¨ì¼ forward pass ì‹¤í–‰
      4. ê²°ê³¼ë¥¼ ê° jobì˜ InferenceResultë¡œ ì €ì¥ + COMPLETED ì²˜ë¦¬
      5. ì´ë¯¸ì§€ ì—†ê±°ë‚˜ ì „ì²˜ë¦¬ ì‹¤íŒ¨í•œ job -> ì¬ì‹œë„ ë˜ëŠ” FAILED ì²˜ë¦¬
    """
    loader = get_loader()

    # 1. DBì—ì„œ Job ê°ì²´ ì¼ê´„ ì¡°íšŒ
    jobs = {
        job.id: job
        for job in InferenceJob.objects.filter(pk__in=job_ids)
    }

    # DBì— ì—†ëŠ” job_id ê²½ê³  (ì´ë¯¸ ì‚­ì œëœ ê²½ìš° ë“±)
    for jid in job_ids:
        if jid not in jobs:
            logger.warning(f"âš ï¸ Job {jid} DBì— ì—†ìŒ, ìŠ¤í‚µ")

    if not jobs:
        return

    # 2. ëª¨ë“  Job ìƒíƒœë¥¼ IN_PROGRESSë¡œ ì¼ê´„ ì—…ë°ì´íŠ¸
    #    update() = SQL UPDATE ... WHERE id IN (...) â€” ë£¨í”„ ì—†ì´ í•œ ì¿¼ë¦¬ë¡œ ì²˜ë¦¬
    InferenceJob.objects.filter(pk__in=list(jobs.keys())).update(
        status=InferenceJob.Status.IN_PROGRESS
    )
    batch_start = time.time()  # ë°°ì¹˜ ì „ì²´ ì²˜ë¦¬ ì‹œì‘ ì‹œê° ê¸°ë¡
    log("batch_start", job_ids=list(jobs.keys()), batch_size=len(jobs))

    # 3. ê° Jobì˜ ì´ë¯¸ì§€ bytesë¥¼ Redisì—ì„œ ê°€ì ¸ì™€ ì „ì²˜ë¦¬
    valid_jobs = []    # ì •ìƒì ìœ¼ë¡œ ì „ì²˜ë¦¬ëœ (job, tensor) ìŒ
    failed_jobs = []   # ì´ë¯¸ì§€ ì—†ê±°ë‚˜ ì „ì²˜ë¦¬ ì‹¤íŒ¨í•œ job

    for job in jobs.values():
        image_bytes = fetch_image_bytes(job.input_sha256)
        if image_bytes is None:
            log("image_not_found", job_id=job.id, reason="redis_expired_or_missing")
            failed_jobs.append(job)
            continue
        try:
            tensor = loader.preprocess(image_bytes)  # bytes -> (1,1,224,224) tensor
            valid_jobs.append((job, tensor))
        except Exception as e:
            log("preprocess_failed", job_id=job.id, error=str(e))
            failed_jobs.append(job)

    # 4. ìœ íš¨í•œ jobë“¤ì„ ë°°ì¹˜ ì¶”ë¡ 
    if valid_jobs:
        tensors = [t for _, t in valid_jobs]  # tensor ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ

        # ë°°ì¹˜ ì¶”ë¡  íƒ€ì„ì•„ì›ƒ = ë‹¨ì¼ íƒ€ì„ì•„ì›ƒ Ã— ë°°ì¹˜ í¬ê¸°
        # (ë°°ì¹˜ê°€ í´ìˆ˜ë¡ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ë¹„ë¡€í•˜ì—¬ ì—¬ìœ ë¥¼ ì¤Œ)
        signal.alarm(settings.INFERENCE_TIMEOUT * len(tensors))
        try:
            # í•µì‹¬: Nê°œ í…ì„œë¥¼ (N,1,224,224)ë¡œ ë¬¶ì–´ í•œ ë²ˆì˜ forward pass ì‹¤í–‰
            batch_scores = loader.predict_batch(tensors)
        except TimeoutError:
            # ë°°ì¹˜ ì „ì²´ íƒ€ì„ì•„ì›ƒ -> ì „ë¶€ ê°œë³„ ì¬ì‹œë„ ëŒ€ìƒìœ¼ë¡œ ì´ë™
            log("inference_timeout", job_ids=[job.id for job, _ in valid_jobs])
            failed_jobs.extend(job for job, _ in valid_jobs)
            valid_jobs = []
            batch_scores = []
        except Exception as e:
            log("inference_error", job_ids=[job.id for job, _ in valid_jobs], error=str(e))
            failed_jobs.extend(job for job, _ in valid_jobs)
            valid_jobs = []
            batch_scores = []
        finally:
            signal.alarm(0)  # íƒ€ì´ë¨¸ í•´ì œ

        # 5. ë°°ì¹˜ ì¶”ë¡  ì„±ê³µí•œ jobë“¤ ê²°ê³¼ ì €ì¥
        for (job, _), scores in zip(valid_jobs, batch_scores):
            top_label = max(scores, key=lambda k: scores[k])
            InferenceResult.objects.create(job=job, output=scores, top_label=top_label)
            job.status = InferenceJob.Status.COMPLETED
            job.save(update_fields=["status", "updated_at"])
            latency_ms = round((time.time() - batch_start) * 1000, 1)
            log("inference_completed", job_id=job.id, top_label=top_label, latency_ms=latency_ms)

    # 6. ì‹¤íŒ¨ jobë“¤: ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬ í›„ ì²˜ë¦¬
    #    ì¬ì‹œë„ íšŸìˆ˜ëŠ” Redisì— ì¹´ìš´í„°ë¡œ ê´€ë¦¬ (DB ì¶”ê°€ ì»¬ëŸ¼ ì—†ì´)
    if failed_jobs:
        _handle_failed_jobs(failed_jobs)


def _handle_failed_jobs(jobs: list) -> None:
    """
    ì‹¤íŒ¨í•œ jobë“¤ì˜ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ Redis ì¹´ìš´í„°ë¡œ ì¶”ì .
    MAX_RETRIES ë¯¸ë§Œì´ë©´ íì— ì¬ë“±ë¡, ì´ˆê³¼ ì‹œ FAILED ì²˜ë¦¬.

    Redis ì¬ì‹œë„ ì¹´ìš´í„° í‚¤: retry:{job_id}  (TTL 1ì‹œê°„)
    """
    from workers.redis_queue import enqueue
    r = redis_lib.from_url(REDIS_URL, decode_responses=True)

    for job in jobs:
        retry_key = f"retry:{job.id}"
        # INCR: ì¹´ìš´í„° ì—†ìœ¼ë©´ 0ì—ì„œ ì‹œì‘í•´ 1 ì¦ê°€, ìˆìœ¼ë©´ +1
        attempt = r.incr(retry_key)
        r.expire(retry_key, 3600)  # 1ì‹œê°„ í›„ ìë™ ì‚­ì œ

        if attempt <= settings.MAX_RETRIES:
            # ì¬ì‹œë„ ê°€ëŠ¥ -> í ë§¨ ë’¤ì— ë‹¤ì‹œ ë“±ë¡
            log("job_retry", job_id=job.id, attempt=f"{attempt}/{settings.MAX_RETRIES}")
            enqueue(job.id)
        else:
            # ì¬ì‹œë„ íšŸìˆ˜ ì†Œì§„ -> FAILED í™•ì •
            job.status = InferenceJob.Status.FAILED
            job.save(update_fields=["status", "updated_at"])
            r.delete(retry_key)  # ì¹´ìš´í„° ì •ë¦¬
            # Dead Letter Queueì— job_id ë³´ê´€ (ìš´ì˜ìê°€ ë‚˜ì¤‘ì— í™•ì¸/ì¬ì²˜ë¦¬ ê°€ëŠ¥)
            # DLQëŠ” ì˜êµ¬ ë³´ê´€ â€” TTL ì—†ìŒ
            r.lpush(DLQ_KEY, job.id)
            log("job_failed", job_id=job.id, max_retries=settings.MAX_RETRIES, dlq=DLQ_KEY)


# â”€â”€ ì›Œì»¤ ë©”ì¸ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_worker():
    """
    ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ì˜ ë©”ì¸ ë£¨í”„.
    ëª¨ë¸ ë¡œë“œ -> Redis í ë°°ì¹˜ í´ë§ -> ë°°ì¹˜ ì¶”ë¡  ë°˜ë³µ.
    SIGTERM ìˆ˜ì‹  ì‹œ í˜„ì¬ ë°°ì¹˜ ì™„ë£Œ í›„ ì¢…ë£Œ (Graceful Shutdown).
    """
    shutdown = False

    def handle_sigterm(signum, frame):
        nonlocal shutdown
        logger.info("âš ï¸ SIGTERM ìˆ˜ì‹  â€” í˜„ì¬ ë°°ì¹˜ ì™„ë£Œ í›„ ì¢…ë£Œ")
        shutdown = True

    signal.signal(signal.SIGTERM, handle_sigterm)

    # ëª¨ë¸ ë¡œë“œ (HuggingFace ìºì‹œ ë˜ëŠ” ë‹¤ìš´ë¡œë“œ í›„ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¼)
    loader = get_loader()
    loader.load()
    logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ â€” Worker ì¤€ë¹„ ã„±ã„±")

    while not shutdown:
        # 30ms ìœˆë„ìš°ë¡œ ë°°ì¹˜ ìˆ˜ì§‘ (ìµœëŒ€ 8ê°œ)
        # íê°€ ë¹„ë©´ BRPOPì´ 5ì´ˆ ëŒ€ê¸° í›„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        job_ids = collect_batch(
            max_wait_ms=settings.BATCH_WINDOW_MS,
            max_size=8,
        )

        if not job_ids:
            # íê°€ ë¹„ì–´ìˆìŒ â€” shutdown ì—¬ë¶€ ì²´í¬ í›„ ë‹¤ì‹œ ëŒ€ê¸°
            continue

        logger.info(f"ğŸ”¥ Batch ìˆ˜ì§‘: {job_ids}")
        process_batch(job_ids)

    logger.info("âœ… Worker ì •ìƒ ì¢…ë£Œ")


if __name__ == "__main__":
    run_worker()
